{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd3cdf0d",
   "metadata": {},
   "source": [
    "# Named-entity Recognition (NER) \n",
    "\n",
    "Named entity recognition is a fundamental task in information extraction from textual documents. While named entities originally corresponded to real-world entities with names (named entities), this concept has been extended to any type of information: it is possible to extract chemical molecules, product numbers, amounts, addresses, etc. In this practical assignment, we will use several named entity extraction libraries in French on a small corpus. The objective is not to train the best possible model, but to test the use of each of these libraries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e7cf35",
   "metadata": {},
   "source": [
    "## The AdminSet dataset\n",
    "The AdminSet dataset is a corpus of administrative documents in French produced by automatic character recognition and manually annotated with named entities. This corpus is quite difficult because the document recognition process produces noisy text (errors due to layout, recognition, fonts, etc.).\n",
    "\n",
    "The paper describing the dataset is available [here](https://hal.science/hal-04855066v1/file/AdminSet_et_AdminBERT__version___preprint.pdf).\n",
    "\n",
    "The corpus is available on HuggingFace: [Adminset-NER](https://huggingface.co/datasets/taln-ls2n/Adminset-NER)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cdb46b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/onyxia/work/nlpenv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 100%|██████████| 729/729 [00:00<00:00, 39786.70 examples/s]\n",
      "Generating validation split: 100%|██████████| 85/85 [00:00<00:00, 10944.80 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 729\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 85\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset('taln-ls2n/Adminset-NER')\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62af8c6",
   "metadata": {},
   "source": [
    "#### Question\n",
    "> * Compute descriptive statistics on the texts  for each split (train, dev)\n",
    "> * Compute descriptive statistics on the entities for each split (train, dev)\n",
    "> * Compare with the statistics reported in the paper (Table 2)\n",
    "> * Display a couple of random texts with their entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b31e1f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics on the number of token in train and validation : min, max, mean std, median\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4967524b-36e7-480a-be19-5aebba678ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 729/729 [00:00<00:00, 8906.82 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Column([63, 24, 31, 18, 41, ...])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = ds[\"train\"].map(lambda x: {\"length\": len(x[\"tokens\"])})\n",
    "temp[\"length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0286324b-5cb2-4214-a066-4c5b10624a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 379 63.3676268861454 52.66968106175962 45.0\n"
     ]
    }
   ],
   "source": [
    "t = temp[\"length\"]\n",
    "print(min(t), max(t), np.mean(t), np.std(t), np.median(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7036b7ea-070c-4e6a-bd24-5cf483200a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(46195)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(temp[\"length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc55aa12-1c50-4b3f-a391-3a049ab8bd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 729/729 [00:00<00:00, 8614.28 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Column([3, 5, 5, 2, 2, ...])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = ds[\"train\"].map(lambda x: {\n",
    "    \"length\": len(x[\"ner_tags\"]), \n",
    "    \"nb_ORG\": x[\"ner_tags\"].count(\"B-ORG\")+x[\"ner_tags\"].count(\"I-ORG\"),\n",
    "    \"nb_PER\": x[\"ner_tags\"].count(\"B-PER\")+x[\"ner_tags\"].count(\"I-PER\"),\n",
    "    \"nb_LOC\": x[\"ner_tags\"].count(\"B-LOC\")+x[\"ner_tags\"].count(\"I-LOC\")\n",
    "})\n",
    "temp[\"nb_PER\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ec1b1ec-fa2f-40f7-b857-aa0cf6529982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 35 3.0809327846364885 4.816555948106844 1.0 2246\n"
     ]
    }
   ],
   "source": [
    "t = temp[\"nb_ORG\"]\n",
    "print(min(t), max(t), np.mean(t), np.std(t), np.median(t), np.sum(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d18be63f-5c58-46af-8867-43a090f8439d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fin', 'O'),\n",
       " ('Procès-Verbal', 'O'),\n",
       " ('Conseil', 'O'),\n",
       " ('communautaire', 'O'),\n",
       " ('du', 'O'),\n",
       " ('lundi', 'O'),\n",
       " ('19', 'O'),\n",
       " ('juin', 'O'),\n",
       " ('2023', 'O'),\n",
       " (\"L'an\", 'O'),\n",
       " ('deux', 'O'),\n",
       " ('mille', 'O'),\n",
       " ('vingt-trois', 'O'),\n",
       " (',', 'O'),\n",
       " ('le', 'O'),\n",
       " ('lundi', 'O'),\n",
       " ('19', 'O'),\n",
       " ('juin', 'O'),\n",
       " (',', 'O'),\n",
       " ('à', 'O'),\n",
       " ('18', 'O'),\n",
       " ('heures', 'O'),\n",
       " ('30', 'O'),\n",
       " (',', 'O'),\n",
       " ('le', 'O'),\n",
       " ('conseil', 'O'),\n",
       " ('communautaire', 'O'),\n",
       " (\"s'est\", 'O'),\n",
       " ('réuni', 'O'),\n",
       " ('à', 'O'),\n",
       " ('Coucy', 'B-LOC'),\n",
       " ('le', 'I-LOC'),\n",
       " ('Château', 'I-LOC'),\n",
       " ('conformément', 'O'),\n",
       " ('à', 'O'),\n",
       " (\"l'article\", 'O'),\n",
       " ('2122-17', 'O'),\n",
       " ('du', 'O'),\n",
       " ('Code', 'O'),\n",
       " ('général', 'O'),\n",
       " ('des', 'O'),\n",
       " ('Collectivités', 'O'),\n",
       " ('Territoriales', 'O'),\n",
       " ('sur', 'O'),\n",
       " ('la', 'O'),\n",
       " ('convocation', 'O'),\n",
       " ('de', 'O'),\n",
       " ('Monsieur', 'B-PER'),\n",
       " ('Vincent', 'I-PER'),\n",
       " ('MORLET', 'I-PER'),\n",
       " (',', 'O'),\n",
       " ('Président', 'O'),\n",
       " (',', 'O'),\n",
       " ('adressée', 'O'),\n",
       " ('aux', 'O'),\n",
       " ('délégués', 'O'),\n",
       " ('des', 'O'),\n",
       " ('communes', 'O'),\n",
       " ('le', 'O'),\n",
       " ('lundi', 'O'),\n",
       " ('12', 'O'),\n",
       " ('juin', 'O'),\n",
       " ('2023.', 'O')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(ds[\"train\"][0][\"tokens\"],ds[\"train\"][0][\"ner_tags\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40f7948d-3377-4d2a-9e9b-a32096b93268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "583"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f34a3d-d5a7-45b3-ab2d-6cf0ae9d813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in ds_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ebbac3",
   "metadata": {},
   "source": [
    "### Creation of the splits\n",
    "\n",
    "The train_test_split() function from huggingface allow to split a dataset randomly in 2 parts : https://huggingface.co/docs/datasets/v4.5.0/process#split\n",
    "\n",
    "The ```spacy_utils.py``` file contains functions to save a dataset in text format (```save_text```, usefull for inspection), BIO format (```save_bio```) and spacy format (```save_docbin```).\n",
    "\n",
    "#### Questions\n",
    ">* Using the split function, create a train/dev/test split corresponding to the proportions reported in the paper\n",
    ">* Save the sets in a corpus directory, in text, bio and docbin formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07bc8ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy_utils import save_bio, save_text, save_docbin\n",
    "from spacy.tokens import Doc, DocBin\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac53e64f-02a0-4020-a9da-25bff1a0c900",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = ds[\"train\"].train_test_split(test_size=0.2)\n",
    "ds_train = temp[\"train\"]\n",
    "ds_dev = temp[\"test\"]\n",
    "ds_test = ds[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ab8a43e-3b89-4880-8143-88f326fb80ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving text to corpus/train.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 583/583 [00:00<00:00, 3163.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/train.txt\n",
      "Saving BIO text to corpus/train.bio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 583/583 [00:00<00:00, 4503.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/train.bio\n",
      "Creating corpus/train.docbin with 583 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 583/583 [00:00<00:00, 1252.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/train.docbin\n",
      "Saving text to corpus/dev.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:00<00:00, 3213.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/dev.txt\n",
      "Saving BIO text to corpus/dev.bio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:00<00:00, 4021.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/dev.bio\n",
      "Creating corpus/dev.docbin with 146 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:00<00:00, 829.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/dev.docbin\n",
      "Saving text to corpus/test.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [00:00<00:00, 3843.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/test.txt\n",
      "Saving BIO text to corpus/test.bio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [00:00<00:00, 5178.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/test.bio\n",
      "Creating corpus/test.docbin with 85 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [00:00<00:00, 797.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/test.docbin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy_utils\n",
    "def aux(d,output_path):\n",
    "    save_text(d,\"corpus/\"+output_path+\".txt\")\n",
    "    save_bio(d,\"corpus/\"+output_path+\".bio\")\n",
    "    save_docbin(d,\"corpus/\"+output_path+\".docbin\")\n",
    "aux(ds_train,\"train\")\n",
    "aux(ds_dev,\"dev\")\n",
    "aux(ds_test,\"test\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6c20ac9-6be6-4384-b48d-c15ace4b2474",
   "metadata": {},
   "source": [
    "import spacy\n",
    "nlp = spacy.blank(\"en\")\n",
    "docbin = DocBin()\n",
    "words = [\"Apple\", \"is\", \"looking\", \"at\", \"buying\", \"U.K.\", \"startup\", \".\"]\n",
    "spaces = [True, True, True, True, True, True, True, False]\n",
    "ents = [\"B-ORG\", \"O\", \"O\", \"O\", \"O\", \"B-GPE\", \"O\", \"O\"]\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces, ents=ents)\n",
    "docbin.add(doc)\n",
    "docbin.to_disk(\"./train.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b05a4b",
   "metadata": {},
   "source": [
    "### Testing spaCy pre-trained NER models\n",
    "\n",
    "spaCy comes with a several pretrained models for many languages. For French, 4 models are provided : https://spacy.io/models/fr\n",
    "\n",
    "To apply a pretrained model to dataset, use : \n",
    "- ```nlp = spacy.load(MODEL_NAME)``` to load the model. You need to download it first with \"spacy download MODEL_NAME\"\n",
    "- ```DocBin().from_disk()``` to load a dataset in spaCy format from the disk\n",
    "- ```doc_bin.get_docs(nlp.vocab)``` to convert the dataset from binary to text format\n",
    "- ```nlp(doc.text)```to apply the NER model to a text\n",
    "\n",
    "To evaluate the prediction, you can use the spaCy [Scorer](https://spacy.io/api/scorer)\n",
    "- ```scorer.score(examples)``` where examples is a list of spaCy ```Example(prediction, reference)````\n",
    "\n",
    "#### Question\n",
    "\n",
    ">* Using a spaCy pretrained model for French, evaluate its performace for NER prediction on the train, dev and test sets\n",
    ">* Compare this model to results reported in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "642b8524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.training import Example\n",
    "from tqdm import tqdm\n",
    "from prettytable import PrettyTable # optional but nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0df8d7e4-7aba-4867-a578-a53b5b08b50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m spacy download fr_core_news_sm\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "doc_bin = DocBin().from_disk(\"corpus/train.docbin\")\n",
    "doc_list = list(doc_bin.get_docs(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4a7d23ea-6a07-46ca-b46a-c871e939ea3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc_list[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4d2c9a88-c399-4ebc-a834-44ba7e968ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "toto = nlp(doc_list[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8a115e97-bc2f-46c2-ae0b-75b9683f497d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il il \n",
      "développe développer \n",
      "des un \n",
      "actions action \n",
      "et et \n",
      "des un \n",
      "projets projet \n",
      "autour autour \n",
      "de de \n",
      "l' le \n",
      "image image \n",
      ", , \n",
      "du de \n",
      "développement développement \n",
      "durable durable \n",
      ", , \n",
      "de de \n",
      "l' le \n",
      "aménagement aménagement \n",
      "urbain urbain \n",
      ", , \n",
      "du de \n",
      "patrimoine patrimoine \n",
      ", , \n",
      "etc. etc. \n",
      "Le le ORG\n",
      "Comité comité ORG\n",
      "engage engager \n",
      "notamment notamment \n",
      "des un \n",
      "actions action \n",
      "de de \n",
      "coopération coopération \n",
      "décentralisée décentraliser \n",
      "entre entrer \n",
      "Angoulême angoulême LOC\n",
      "et et \n",
      "Ségou Ségou LOC\n",
      "Mali Mali LOC\n",
      ", , \n",
      "contribuant contribuer \n",
      "ainsi ainsi \n",
      "au au \n",
      "rayonnement rayonnement \n",
      "international international \n",
      "de de \n",
      "la le LOC\n",
      "Ville ville LOC\n",
      ". . \n"
     ]
    }
   ],
   "source": [
    "for token in toto:\n",
    "    print(token.text, token.lemma_, token.ent_type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9f1fadc3-b65e-4bce-ab6d-fd8c1983e8af",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il\t\t\n",
      "développe\t\t\n",
      "des\t\t\n",
      "actions\t\t\n",
      "et\t\t\n",
      "des\t\t\n",
      "projets\t\t\n",
      "autour\t\t\n",
      "de\t\t\n",
      "l'\t\t\n",
      "image\t\t\n",
      ",\t\t\n",
      "du\t\t\n",
      "développement\t\t\n",
      "durable\t\t\n",
      ",\t\t\n",
      "de\t\t\n",
      "l'\t\t\n",
      "aménagement\t\t\n",
      "urbain\t\t\n",
      ",\t\t\n",
      "du\t\t\n",
      "patrimoine\t\t\n",
      ",\t\tORG\n",
      "etc.\t\tORG\n",
      "Le\tORG\t\n",
      "Comité\tORG\t\n",
      "engage\t\t\n",
      "notamment\t\t\n",
      "des\t\t\n",
      "actions\t\t\n",
      "de\t\t\n",
      "coopération\t\t\n",
      "décentralisée\t\tORG\n",
      "entre\t\t\n",
      "Angoulême\tLOC\tORG\n",
      "et\t\tORG\n",
      "Ségou\tLOC\t\n",
      "Mali\tLOC\t\n",
      ",\t\t\n",
      "contribuant\t\t\n",
      "ainsi\t\t\n",
      "au\t\t\n",
      "rayonnement\t\t\n",
      "international\t\tLOC\n",
      "de\t\tLOC\n",
      "la\tLOC\t\n"
     ]
    }
   ],
   "source": [
    "for (pred,ref) in zip(toto,doc_list[0]):\n",
    "    print(pred.text, pred.ent_type_, ref.ent_type_, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "170a3416-97c8-4035-8a38-42d6cd508537",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = list(map(lambda d: nlp(d.text), doc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ed4b2431-a763-429b-acaf-c3e874ba4d4e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "il \n",
      "développer \n",
      "un \n",
      "action \n",
      "et \n",
      "un \n",
      "projet \n",
      "autour \n",
      "de \n",
      "l'image \n",
      ", \n",
      "de \n",
      "développement \n",
      "durable \n",
      ", \n",
      "de \n",
      "l'aménagement \n",
      "urbain \n",
      ", \n",
      "de \n",
      "patrimoine \n",
      ", \n",
      "etc. \n",
      "le ORG\n",
      "comité ORG\n",
      "engager \n",
      "notamment \n",
      "un \n",
      "action \n",
      "de \n",
      "coopération \n",
      "décentraliser \n",
      "entrer \n",
      "angoulême ORG\n",
      "et \n",
      "Ségou ORG\n",
      "Mali ORG\n",
      ", \n",
      "contribuer \n",
      "ainsi \n",
      "au \n",
      "rayonnement \n",
      "international \n",
      "de \n",
      "le LOC\n",
      "ville LOC\n",
      ". \n"
     ]
    }
   ],
   "source": [
    "for token in predictions[0]:\n",
    "    print(token.lemma_, token.ent_type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7815974c-56df-4aa0-92da-362da4c10d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f390b09d-7a6d-419b-9ce3-c1dc73ee0e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = Scorer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cb4a3520-23ff-4bcc-bf43-c1f74d614d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = list(map(Example, predictions,doc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a250b186-50dd-4fd5-845a-4f4bcc8471f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': None,\n",
       " 'token_p': None,\n",
       " 'token_r': None,\n",
       " 'token_f': None,\n",
       " 'pos_acc': 0.9411811367151866,\n",
       " 'morph_acc': 0.9413118831529648,\n",
       " 'morph_micro_p': 0.9524083734164352,\n",
       " 'morph_micro_r': 0.9696854738584633,\n",
       " 'morph_micro_f': 0.9609692744112923,\n",
       " 'morph_per_feat': {'Gender': {'p': 0.9605792437650845,\n",
       "   'r': 0.968931138418734,\n",
       "   'f': 0.964737115484504},\n",
       "  'Number': {'p': 0.9851409389346892,\n",
       "   'r': 0.9802053146028783,\n",
       "   'f': 0.98266692928895},\n",
       "  'Person': {'p': 0.904603068712475,\n",
       "   'r': 0.900398406374502,\n",
       "   'f': 0.9024958402662229},\n",
       "  'Mood': {'p': 0.9162348877374784,\n",
       "   'r': 0.9107296137339056,\n",
       "   'f': 0.9134739560912613},\n",
       "  'Tense': {'p': 0.9436201780415431,\n",
       "   'r': 0.9228855721393034,\n",
       "   'f': 0.9331377069796688},\n",
       "  'VerbForm': {'p': 0.9548599670510708,\n",
       "   'r': 0.9330328396651641,\n",
       "   'f': 0.943820224719101},\n",
       "  'Definite': {'p': 0.8876337693222355,\n",
       "   'r': 0.992686170212766,\n",
       "   'f': 0.9372253609541745},\n",
       "  'PronType': {'p': 0.893644617380026,\n",
       "   'r': 0.9891304347826086,\n",
       "   'f': 0.9389662221356955},\n",
       "  'Poss': {'p': 0.9836956521739131,\n",
       "   'r': 0.9836956521739131,\n",
       "   'f': 0.9836956521739131},\n",
       "  'NumType': {'p': 0.9539473684210527,\n",
       "   'r': 0.9823848238482384,\n",
       "   'f': 0.9679572763684913},\n",
       "  'Voice': {'p': 0.9424460431654677,\n",
       "   'r': 0.9597069597069597,\n",
       "   'f': 0.9509981851179674},\n",
       "  'Reflex': {'p': 0.423728813559322,\n",
       "   'r': 0.44642857142857145,\n",
       "   'f': 0.4347826086956522},\n",
       "  'Polarity': {'p': 0.8444444444444444,\n",
       "   'r': 0.987012987012987,\n",
       "   'f': 0.9101796407185628}},\n",
       " 'sents_p': 0.7756017951856385,\n",
       " 'sents_r': 0.7289110429447853,\n",
       " 'sents_f': 0.751531923305001,\n",
       " 'dep_uas': 0.8900038254421329,\n",
       " 'dep_las': 0.8555641636578913,\n",
       " 'dep_las_per_type': {'nsubj': {'p': 0.765034965034965,\n",
       "   'r': 0.7985401459854015,\n",
       "   'f': 0.7814285714285715},\n",
       "  'root': {'p': 0.8389441469013007,\n",
       "   'r': 0.8408742331288344,\n",
       "   'f': 0.8399080811949445},\n",
       "  'det': {'p': 0.835081727250549,\n",
       "   'r': 0.9782795084309803,\n",
       "   'f': 0.9010265859436694},\n",
       "  'obj': {'p': 0.7970244420828906,\n",
       "   'r': 0.8503401360544217,\n",
       "   'f': 0.8228195282501372},\n",
       "  'cc': {'p': 0.9600977198697068,\n",
       "   'r': 0.9616639477977161,\n",
       "   'f': 0.9608801955990219},\n",
       "  'conj': {'p': 0.775886524822695,\n",
       "   'r': 0.7698803659394793,\n",
       "   'f': 0.7728717767573297},\n",
       "  'dep': {'p': 0.4793103448275862,\n",
       "   'r': 0.5720164609053497,\n",
       "   'f': 0.5215759849906191},\n",
       "  'case': {'p': 0.9252909720037747,\n",
       "   'r': 0.9769179674526736,\n",
       "   'f': 0.9504038772213247},\n",
       "  'obl:arg': {'p': 0.8231780167264038,\n",
       "   'r': 0.8412698412698413,\n",
       "   'f': 0.8321256038647342},\n",
       "  'amod': {'p': 0.8530785562632697,\n",
       "   'r': 0.8294797687861272,\n",
       "   'f': 0.8411136696671551},\n",
       "  'mark': {'p': 0.8296422487223168,\n",
       "   'r': 0.9171374764595104,\n",
       "   'f': 0.8711985688729875},\n",
       "  'nmod': {'p': 0.811816192560175,\n",
       "   'r': 0.8629244844161885,\n",
       "   'f': 0.8365904990980156},\n",
       "  'advmod': {'p': 0.7545787545787546,\n",
       "   'r': 0.7907869481765835,\n",
       "   'f': 0.7722586691658857},\n",
       "  'acl': {'p': 0.7651612903225806,\n",
       "   'r': 0.7751633986928105,\n",
       "   'f': 0.7701298701298701},\n",
       "  'obl:mod': {'p': 0.7099236641221374,\n",
       "   'r': 0.7390728476821192,\n",
       "   'f': 0.7242050616482804},\n",
       "  'flat:name': {'p': 0.7916230366492146, 'r': 0.864, 'f': 0.8262295081967213},\n",
       "  'appos': {'p': 0.6915422885572139,\n",
       "   'r': 0.7354497354497355,\n",
       "   'f': 0.7128205128205128},\n",
       "  'cop': {'p': 0.8145161290322581,\n",
       "   'r': 0.9099099099099099,\n",
       "   'f': 0.8595744680851065},\n",
       "  'nsubj:pass': {'p': 0.8755555555555555,\n",
       "   'r': 0.9036697247706422,\n",
       "   'f': 0.8893905191873589},\n",
       "  'nummod': {'p': 0.8799668874172185,\n",
       "   'r': 0.9069965870307167,\n",
       "   'f': 0.8932773109243698},\n",
       "  'aux:tense': {'p': 0.8531468531468531,\n",
       "   'r': 0.7973856209150327,\n",
       "   'f': 0.8243243243243243},\n",
       "  'aux:pass': {'p': 0.9651162790697675,\n",
       "   'r': 0.9803149606299213,\n",
       "   'f': 0.97265625},\n",
       "  'obl:agent': {'p': 0.8938053097345132,\n",
       "   'r': 0.8632478632478633,\n",
       "   'f': 0.8782608695652174},\n",
       "  'acl:relcl': {'p': 0.6993464052287581,\n",
       "   'r': 0.722972972972973,\n",
       "   'f': 0.7109634551495018},\n",
       "  'xcomp': {'p': 0.7787356321839081,\n",
       "   'r': 0.811377245508982,\n",
       "   'f': 0.7947214076246335},\n",
       "  'expl:pass': {'p': 0.375, 'r': 0.5, 'f': 0.42857142857142855},\n",
       "  'ccomp': {'p': 0.46511627906976744,\n",
       "   'r': 0.7692307692307693,\n",
       "   'f': 0.5797101449275363},\n",
       "  'advcl': {'p': 0.6045197740112994,\n",
       "   'r': 0.6257309941520468,\n",
       "   'f': 0.6149425287356322},\n",
       "  'iobj': {'p': 0.8732394366197183,\n",
       "   'r': 0.8985507246376812,\n",
       "   'f': 0.8857142857142857},\n",
       "  'fixed': {'p': 0.7559055118110236, 'r': 0.75, 'f': 0.7529411764705882},\n",
       "  'parataxis': {'p': 0.673469387755102, 'r': 0.66, 'f': 0.6666666666666666},\n",
       "  'expl:subj': {'p': 0.49122807017543857,\n",
       "   'r': 0.8484848484848485,\n",
       "   'f': 0.6222222222222222},\n",
       "  'expl:comp': {'p': 0.391304347826087, 'r': 0.9, 'f': 0.5454545454545454},\n",
       "  'vocative': {'p': 0.625, 'r': 0.5555555555555556, 'f': 0.5882352941176471},\n",
       "  'flat:foreign': {'p': 0.16666666666666666,\n",
       "   'r': 0.18181818181818182,\n",
       "   'f': 0.17391304347826086}},\n",
       " 'tag_acc': 0.9411811367151866,\n",
       " 'lemma_acc': 0.9057236850728076,\n",
       " 'ents_p': 0.56601607347876,\n",
       " 'ents_r': 0.5730892182505086,\n",
       " 'ents_f': 0.5695306859205778,\n",
       " 'ents_per_type': {'ORG': {'p': 0.5675,\n",
       "   'r': 0.442495126705653,\n",
       "   'f': 0.4972617743702081},\n",
       "  'LOC': {'p': 0.5134831460674157,\n",
       "   'r': 0.5836526181353767,\n",
       "   'f': 0.5463239689181113},\n",
       "  'PER': {'p': 0.569164265129683,\n",
       "   'r': 0.47878787878787876,\n",
       "   'f': 0.5200789993416721},\n",
       "  'MISC': {'p': 0.6054545454545455,\n",
       "   'r': 0.8252788104089219,\n",
       "   'f': 0.6984792868379653}}}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer.score(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a358f45",
   "metadata": {},
   "source": [
    "### Training a custom spaCy model\n",
    "\n",
    "The training of a cupstom spaCy NER model can be done both with the command line interface (cli) or in a python script. Using the cli is ususally more optimzed. All the configuration of the training is defined in a coniguration file, which is a good practice for documentation, tracing and reproducibility.\n",
    "\n",
    "The configuration file can be generated on line using the [Quickstart](https://spacy.io/usage/training#quickstart)\n",
    "\n",
    "<img src=\"images/spacy_quickstart.jpg\" width=\"600\" >\n",
    "\n",
    "You can run the training process as a script using the train function (https://spacy.io/usage/training#api-train), specifying the configuration file and the directory in which to save the model as parameters. Once the training is complete, the best and last models are saved in the directory.\n",
    "\n",
    "#### Question\n",
    "> * Generate a training configuration file for a NER in French\n",
    "> * Add the correct path to the training and dev sets generated previously\n",
    "> * train a NER model\n",
    "> * Evaluate the model on the train, dev et test sets. Compare to the results reported in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf90601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "from spacy.cli.train import train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3982808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34932c83",
   "metadata": {},
   "source": [
    "### Zero-shot NER prediction with GLiNER\n",
    "\n",
    "\n",
    "[GLiNER](https://github.com/fastino-ai/GLiNER2/tree/main)  is a library that provides models for zero-shot named entity recognition. This means that[structured information extraction](https://github.com/fastino-ai/GLiNER2/blob/main/tutorial/3-json_extraction.md)structured information extraction, which means that the extracted information can be organised in a structured JSON format. GLiNER does not provide the location of entities in the text by default, but you can configure the model to output this information (```include_spans=True```). Finally, GLiNER enables entities to be overlapped and nested, which is not supported by the spaCy scorer. The spaCy [filter_spans](https://spacy.io/api/top-level#util.filter_spans) function can be used to remove overlapping entities for evaluation.\n",
    "\n",
    "#### Question\n",
    "> * Define the entities to extract from the text.\n",
    "> * Apply GLiNER on the dev and test sets\n",
    "> * Evaluate the models on the dev and test sets and compare to the results reported in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f09c987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gliner2 import GLiNER2\n",
    "extractor = GLiNER2.from_pretrained(\"fastino/gliner2-base-v1\")\n",
    "from spacy.util import filter_spans\n",
    "nlp = spacy.blank(\"fr\")  # tokenizer only\n",
    "\n",
    "doc_bin = DocBin().from_disk(\"REPLACE\")\n",
    "gold_docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "\n",
    "label_map = {\n",
    "    # Define the entities here\n",
    "}\n",
    "\n",
    "gliner_labels = list(label_map.values())\n",
    "reverse_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "examples = []\n",
    "\n",
    "for gold_doc in gold_docs:\n",
    "    text = gold_doc.text\n",
    "    \n",
    "    predictions = extractor.extract_entities(text, gliner_labels, include_spans=True)\n",
    "    pred_doc = nlp.make_doc(text)\n",
    "\n",
    "    spans = []\n",
    "    for gliner_label, entities in predictions[\"entities\"].items():\n",
    "        spacy_label = reverse_map.get(gliner_label)\n",
    "        if not spacy_label:\n",
    "            continue\n",
    "        for ent in entities:\n",
    "            start = ent[\"start\"]\n",
    "            end = ent[\"end\"]\n",
    "\n",
    "            span = pred_doc.char_span(start, end, label=spacy_label)\n",
    "            if span:\n",
    "                spans.append(span)\n",
    "\n",
    "    spans = filter_spans(spans)\n",
    "    pred_doc.ents = spans\n",
    "    examples.append(Example(pred_doc, gold_doc))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "nlpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
