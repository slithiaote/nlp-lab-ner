{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd3cdf0d",
   "metadata": {},
   "source": [
    "# Named-entity Recognition (NER) \n",
    "\n",
    "Named entity recognition is a fundamental task in information extraction from textual documents. While named entities originally corresponded to real-world entities with names (named entities), this concept has been extended to any type of information: it is possible to extract chemical molecules, product numbers, amounts, addresses, etc. In this practical assignment, we will use several named entity extraction libraries in French on a small corpus. The objective is not to train the best possible model, but to test the use of each of these libraries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e7cf35",
   "metadata": {},
   "source": [
    "## The AdminSet dataset\n",
    "The AdminSet dataset is a corpus of administrative documents in French produced by automatic character recognition and manually annotated with named entities. This corpus is quite difficult because the document recognition process produces noisy text (errors due to layout, recognition, fonts, etc.).\n",
    "\n",
    "The paper describing the dataset is available [here](https://hal.science/hal-04855066v1/file/AdminSet_et_AdminBERT__version___preprint.pdf).\n",
    "\n",
    "The corpus is available on HuggingFace: [Adminset-NER](https://huggingface.co/datasets/taln-ls2n/Adminset-NER)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cdb46b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/onyxia/work/nlpenv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 729/729 [00:00<00:00, 39786.70 examples/s]\n",
      "Generating validation split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85/85 [00:00<00:00, 10944.80 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 729\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 85\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset('taln-ls2n/Adminset-NER')\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62af8c6",
   "metadata": {},
   "source": [
    "#### Question\n",
    "> * Compute descriptive statistics on the texts  for each split (train, dev)\n",
    "> * Compute descriptive statistics on the entities for each split (train, dev)\n",
    "> * Compare with the statistics reported in the paper (Table 2)\n",
    "> * Display a couple of random texts with their entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b31e1f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics on the number of token in train and validation : min, max, mean std, median\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4967524b-36e7-480a-be19-5aebba678ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 729/729 [00:00<00:00, 8906.82 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Column([63, 24, 31, 18, 41, ...])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = ds[\"train\"].map(lambda x: {\"length\": len(x[\"tokens\"])})\n",
    "temp[\"length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0286324b-5cb2-4214-a066-4c5b10624a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 379 63.3676268861454 52.66968106175962 45.0\n"
     ]
    }
   ],
   "source": [
    "t = temp[\"length\"]\n",
    "print(min(t), max(t), np.mean(t), np.std(t), np.median(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7036b7ea-070c-4e6a-bd24-5cf483200a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(46195)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(temp[\"length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc55aa12-1c50-4b3f-a391-3a049ab8bd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 729/729 [00:00<00:00, 8614.28 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Column([3, 5, 5, 2, 2, ...])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = ds[\"train\"].map(lambda x: {\n",
    "    \"length\": len(x[\"ner_tags\"]), \n",
    "    \"nb_ORG\": x[\"ner_tags\"].count(\"B-ORG\")+x[\"ner_tags\"].count(\"I-ORG\"),\n",
    "    \"nb_PER\": x[\"ner_tags\"].count(\"B-PER\")+x[\"ner_tags\"].count(\"I-PER\"),\n",
    "    \"nb_LOC\": x[\"ner_tags\"].count(\"B-LOC\")+x[\"ner_tags\"].count(\"I-LOC\")\n",
    "})\n",
    "temp[\"nb_PER\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ec1b1ec-fa2f-40f7-b857-aa0cf6529982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 35 3.0809327846364885 4.816555948106844 1.0 2246\n"
     ]
    }
   ],
   "source": [
    "t = temp[\"nb_ORG\"]\n",
    "print(min(t), max(t), np.mean(t), np.std(t), np.median(t), np.sum(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d18be63f-5c58-46af-8867-43a090f8439d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fin', 'O'),\n",
       " ('Proc√®s-Verbal', 'O'),\n",
       " ('Conseil', 'O'),\n",
       " ('communautaire', 'O'),\n",
       " ('du', 'O'),\n",
       " ('lundi', 'O'),\n",
       " ('19', 'O'),\n",
       " ('juin', 'O'),\n",
       " ('2023', 'O'),\n",
       " (\"L'an\", 'O'),\n",
       " ('deux', 'O'),\n",
       " ('mille', 'O'),\n",
       " ('vingt-trois', 'O'),\n",
       " (',', 'O'),\n",
       " ('le', 'O'),\n",
       " ('lundi', 'O'),\n",
       " ('19', 'O'),\n",
       " ('juin', 'O'),\n",
       " (',', 'O'),\n",
       " ('√†', 'O'),\n",
       " ('18', 'O'),\n",
       " ('heures', 'O'),\n",
       " ('30', 'O'),\n",
       " (',', 'O'),\n",
       " ('le', 'O'),\n",
       " ('conseil', 'O'),\n",
       " ('communautaire', 'O'),\n",
       " (\"s'est\", 'O'),\n",
       " ('r√©uni', 'O'),\n",
       " ('√†', 'O'),\n",
       " ('Coucy', 'B-LOC'),\n",
       " ('le', 'I-LOC'),\n",
       " ('Ch√¢teau', 'I-LOC'),\n",
       " ('conform√©ment', 'O'),\n",
       " ('√†', 'O'),\n",
       " (\"l'article\", 'O'),\n",
       " ('2122-17', 'O'),\n",
       " ('du', 'O'),\n",
       " ('Code', 'O'),\n",
       " ('g√©n√©ral', 'O'),\n",
       " ('des', 'O'),\n",
       " ('Collectivit√©s', 'O'),\n",
       " ('Territoriales', 'O'),\n",
       " ('sur', 'O'),\n",
       " ('la', 'O'),\n",
       " ('convocation', 'O'),\n",
       " ('de', 'O'),\n",
       " ('Monsieur', 'B-PER'),\n",
       " ('Vincent', 'I-PER'),\n",
       " ('MORLET', 'I-PER'),\n",
       " (',', 'O'),\n",
       " ('Pr√©sident', 'O'),\n",
       " (',', 'O'),\n",
       " ('adress√©e', 'O'),\n",
       " ('aux', 'O'),\n",
       " ('d√©l√©gu√©s', 'O'),\n",
       " ('des', 'O'),\n",
       " ('communes', 'O'),\n",
       " ('le', 'O'),\n",
       " ('lundi', 'O'),\n",
       " ('12', 'O'),\n",
       " ('juin', 'O'),\n",
       " ('2023.', 'O')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(ds[\"train\"][0][\"tokens\"],ds[\"train\"][0][\"ner_tags\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40f7948d-3377-4d2a-9e9b-a32096b93268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "583"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f34a3d-d5a7-45b3-ab2d-6cf0ae9d813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in ds_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ebbac3",
   "metadata": {},
   "source": [
    "### Creation of the splits\n",
    "\n",
    "The train_test_split() function from huggingface allow to split a dataset randomly in 2 parts : https://huggingface.co/docs/datasets/v4.5.0/process#split\n",
    "\n",
    "The ```spacy_utils.py``` file contains functions to save a dataset in text format (```save_text```, usefull for inspection), BIO format (```save_bio```) and spacy format (```save_docbin```).\n",
    "\n",
    "#### Questions\n",
    ">* Using the split function, create a train/dev/test split corresponding to the proportions reported in the paper\n",
    ">* Save the sets in a corpus directory, in text, bio and docbin formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07bc8ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy_utils import save_bio, save_text, save_docbin\n",
    "from spacy.tokens import Doc, DocBin\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac53e64f-02a0-4020-a9da-25bff1a0c900",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = ds[\"train\"].train_test_split(test_size=0.2)\n",
    "ds_train = temp[\"train\"]\n",
    "ds_dev = temp[\"test\"]\n",
    "ds_test = ds[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ab8a43e-3b89-4880-8143-88f326fb80ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving text to corpus/train.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 583/583 [00:00<00:00, 3163.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/train.txt\n",
      "Saving BIO text to corpus/train.bio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 583/583 [00:00<00:00, 4503.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/train.bio\n",
      "Creating corpus/train.docbin with 583 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 583/583 [00:00<00:00, 1252.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/train.docbin\n",
      "Saving text to corpus/dev.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 146/146 [00:00<00:00, 3213.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/dev.txt\n",
      "Saving BIO text to corpus/dev.bio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 146/146 [00:00<00:00, 4021.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/dev.bio\n",
      "Creating corpus/dev.docbin with 146 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 146/146 [00:00<00:00, 829.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/dev.docbin\n",
      "Saving text to corpus/test.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85/85 [00:00<00:00, 3843.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/test.txt\n",
      "Saving BIO text to corpus/test.bio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85/85 [00:00<00:00, 5178.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/test.bio\n",
      "Creating corpus/test.docbin with 85 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85/85 [00:00<00:00, 797.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to corpus/test.docbin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy_utils\n",
    "def aux(d,output_path):\n",
    "    save_text(d,\"corpus/\"+output_path+\".txt\")\n",
    "    save_bio(d,\"corpus/\"+output_path+\".bio\")\n",
    "    save_docbin(d,\"corpus/\"+output_path+\".docbin\")\n",
    "aux(ds_train,\"train\")\n",
    "aux(ds_dev,\"dev\")\n",
    "aux(ds_test,\"test\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6c20ac9-6be6-4384-b48d-c15ace4b2474",
   "metadata": {},
   "source": [
    "import spacy\n",
    "nlp = spacy.blank(\"en\")\n",
    "docbin = DocBin()\n",
    "words = [\"Apple\", \"is\", \"looking\", \"at\", \"buying\", \"U.K.\", \"startup\", \".\"]\n",
    "spaces = [True, True, True, True, True, True, True, False]\n",
    "ents = [\"B-ORG\", \"O\", \"O\", \"O\", \"O\", \"B-GPE\", \"O\", \"O\"]\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces, ents=ents)\n",
    "docbin.add(doc)\n",
    "docbin.to_disk(\"./train.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b05a4b",
   "metadata": {},
   "source": [
    "### Testing spaCy pre-trained NER models\n",
    "\n",
    "spaCy comes with a several pretrained models for many languages. For French, 4 models are provided : https://spacy.io/models/fr\n",
    "\n",
    "To apply a pretrained model to dataset, use : \n",
    "- ```nlp = spacy.load(MODEL_NAME)``` to load the model. You need to download it first with \"spacy download MODEL_NAME\"\n",
    "- ```DocBin().from_disk()``` to load a dataset in spaCy format from the disk\n",
    "- ```doc_bin.get_docs(nlp.vocab)``` to convert the dataset from binary to text format\n",
    "- ```nlp(doc.text)```to apply the NER model to a text\n",
    "\n",
    "To evaluate the prediction, you can use the spaCy [Scorer](https://spacy.io/api/scorer)\n",
    "- ```scorer.score(examples)``` where examples is a list of spaCy ```Example(prediction, reference)````\n",
    "\n",
    "#### Question\n",
    "\n",
    ">* Using a spaCy pretrained model for French, evaluate its performace for NER prediction on the train, dev and test sets\n",
    ">* Compare this model to results reported in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "642b8524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.training import Example\n",
    "from tqdm import tqdm\n",
    "from prettytable import PrettyTable # optional but nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0df8d7e4-7aba-4867-a578-a53b5b08b50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m spacy download fr_core_news_sm\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "doc_bin = DocBin().from_disk(\"corpus/train.docbin\")\n",
    "doc_list = list(doc_bin.get_docs(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4a7d23ea-6a07-46ca-b46a-c871e939ea3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc_list[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4d2c9a88-c399-4ebc-a834-44ba7e968ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "toto = nlp(doc_list[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8a115e97-bc2f-46c2-ae0b-75b9683f497d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il il \n",
      "d√©veloppe d√©velopper \n",
      "des un \n",
      "actions action \n",
      "et et \n",
      "des un \n",
      "projets projet \n",
      "autour autour \n",
      "de de \n",
      "l' le \n",
      "image image \n",
      ", , \n",
      "du de \n",
      "d√©veloppement d√©veloppement \n",
      "durable durable \n",
      ", , \n",
      "de de \n",
      "l' le \n",
      "am√©nagement am√©nagement \n",
      "urbain urbain \n",
      ", , \n",
      "du de \n",
      "patrimoine patrimoine \n",
      ", , \n",
      "etc. etc. \n",
      "Le le ORG\n",
      "Comit√© comit√© ORG\n",
      "engage engager \n",
      "notamment notamment \n",
      "des un \n",
      "actions action \n",
      "de de \n",
      "coop√©ration coop√©ration \n",
      "d√©centralis√©e d√©centraliser \n",
      "entre entrer \n",
      "Angoul√™me angoul√™me LOC\n",
      "et et \n",
      "S√©gou S√©gou LOC\n",
      "Mali Mali LOC\n",
      ", , \n",
      "contribuant contribuer \n",
      "ainsi ainsi \n",
      "au au \n",
      "rayonnement rayonnement \n",
      "international international \n",
      "de de \n",
      "la le LOC\n",
      "Ville ville LOC\n",
      ". . \n"
     ]
    }
   ],
   "source": [
    "for token in toto:\n",
    "    print(token.text, token.lemma_, token.ent_type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9f1fadc3-b65e-4bce-ab6d-fd8c1983e8af",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il\t\t\n",
      "d√©veloppe\t\t\n",
      "des\t\t\n",
      "actions\t\t\n",
      "et\t\t\n",
      "des\t\t\n",
      "projets\t\t\n",
      "autour\t\t\n",
      "de\t\t\n",
      "l'\t\t\n",
      "image\t\t\n",
      ",\t\t\n",
      "du\t\t\n",
      "d√©veloppement\t\t\n",
      "durable\t\t\n",
      ",\t\t\n",
      "de\t\t\n",
      "l'\t\t\n",
      "am√©nagement\t\t\n",
      "urbain\t\t\n",
      ",\t\t\n",
      "du\t\t\n",
      "patrimoine\t\t\n",
      ",\t\tORG\n",
      "etc.\t\tORG\n",
      "Le\tORG\t\n",
      "Comit√©\tORG\t\n",
      "engage\t\t\n",
      "notamment\t\t\n",
      "des\t\t\n",
      "actions\t\t\n",
      "de\t\t\n",
      "coop√©ration\t\t\n",
      "d√©centralis√©e\t\tORG\n",
      "entre\t\t\n",
      "Angoul√™me\tLOC\tORG\n",
      "et\t\tORG\n",
      "S√©gou\tLOC\t\n",
      "Mali\tLOC\t\n",
      ",\t\t\n",
      "contribuant\t\t\n",
      "ainsi\t\t\n",
      "au\t\t\n",
      "rayonnement\t\t\n",
      "international\t\tLOC\n",
      "de\t\tLOC\n",
      "la\tLOC\t\n"
     ]
    }
   ],
   "source": [
    "for (pred,ref) in zip(toto,doc_list[0]):\n",
    "    print(pred.text, pred.ent_type_, ref.ent_type_, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "170a3416-97c8-4035-8a38-42d6cd508537",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = list(map(lambda d: nlp(d.text), doc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ed4b2431-a763-429b-acaf-c3e874ba4d4e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "il \n",
      "d√©velopper \n",
      "un \n",
      "action \n",
      "et \n",
      "un \n",
      "projet \n",
      "autour \n",
      "de \n",
      "l'image \n",
      ", \n",
      "de \n",
      "d√©veloppement \n",
      "durable \n",
      ", \n",
      "de \n",
      "l'am√©nagement \n",
      "urbain \n",
      ", \n",
      "de \n",
      "patrimoine \n",
      ", \n",
      "etc. \n",
      "le ORG\n",
      "comit√© ORG\n",
      "engager \n",
      "notamment \n",
      "un \n",
      "action \n",
      "de \n",
      "coop√©ration \n",
      "d√©centraliser \n",
      "entrer \n",
      "angoul√™me ORG\n",
      "et \n",
      "S√©gou ORG\n",
      "Mali ORG\n",
      ", \n",
      "contribuer \n",
      "ainsi \n",
      "au \n",
      "rayonnement \n",
      "international \n",
      "de \n",
      "le LOC\n",
      "ville LOC\n",
      ". \n"
     ]
    }
   ],
   "source": [
    "for token in predictions[0]:\n",
    "    print(token.lemma_, token.ent_type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7815974c-56df-4aa0-92da-362da4c10d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f390b09d-7a6d-419b-9ce3-c1dc73ee0e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = Scorer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cb4a3520-23ff-4bcc-bf43-c1f74d614d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = list(map(Example, predictions,doc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a250b186-50dd-4fd5-845a-4f4bcc8471f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': None,\n",
       " 'token_p': None,\n",
       " 'token_r': None,\n",
       " 'token_f': None,\n",
       " 'pos_acc': 0.9411811367151866,\n",
       " 'morph_acc': 0.9413118831529648,\n",
       " 'morph_micro_p': 0.9524083734164352,\n",
       " 'morph_micro_r': 0.9696854738584633,\n",
       " 'morph_micro_f': 0.9609692744112923,\n",
       " 'morph_per_feat': {'Gender': {'p': 0.9605792437650845,\n",
       "   'r': 0.968931138418734,\n",
       "   'f': 0.964737115484504},\n",
       "  'Number': {'p': 0.9851409389346892,\n",
       "   'r': 0.9802053146028783,\n",
       "   'f': 0.98266692928895},\n",
       "  'Person': {'p': 0.904603068712475,\n",
       "   'r': 0.900398406374502,\n",
       "   'f': 0.9024958402662229},\n",
       "  'Mood': {'p': 0.9162348877374784,\n",
       "   'r': 0.9107296137339056,\n",
       "   'f': 0.9134739560912613},\n",
       "  'Tense': {'p': 0.9436201780415431,\n",
       "   'r': 0.9228855721393034,\n",
       "   'f': 0.9331377069796688},\n",
       "  'VerbForm': {'p': 0.9548599670510708,\n",
       "   'r': 0.9330328396651641,\n",
       "   'f': 0.943820224719101},\n",
       "  'Definite': {'p': 0.8876337693222355,\n",
       "   'r': 0.992686170212766,\n",
       "   'f': 0.9372253609541745},\n",
       "  'PronType': {'p': 0.893644617380026,\n",
       "   'r': 0.9891304347826086,\n",
       "   'f': 0.9389662221356955},\n",
       "  'Poss': {'p': 0.9836956521739131,\n",
       "   'r': 0.9836956521739131,\n",
       "   'f': 0.9836956521739131},\n",
       "  'NumType': {'p': 0.9539473684210527,\n",
       "   'r': 0.9823848238482384,\n",
       "   'f': 0.9679572763684913},\n",
       "  'Voice': {'p': 0.9424460431654677,\n",
       "   'r': 0.9597069597069597,\n",
       "   'f': 0.9509981851179674},\n",
       "  'Reflex': {'p': 0.423728813559322,\n",
       "   'r': 0.44642857142857145,\n",
       "   'f': 0.4347826086956522},\n",
       "  'Polarity': {'p': 0.8444444444444444,\n",
       "   'r': 0.987012987012987,\n",
       "   'f': 0.9101796407185628}},\n",
       " 'sents_p': 0.7756017951856385,\n",
       " 'sents_r': 0.7289110429447853,\n",
       " 'sents_f': 0.751531923305001,\n",
       " 'dep_uas': 0.8900038254421329,\n",
       " 'dep_las': 0.8555641636578913,\n",
       " 'dep_las_per_type': {'nsubj': {'p': 0.765034965034965,\n",
       "   'r': 0.7985401459854015,\n",
       "   'f': 0.7814285714285715},\n",
       "  'root': {'p': 0.8389441469013007,\n",
       "   'r': 0.8408742331288344,\n",
       "   'f': 0.8399080811949445},\n",
       "  'det': {'p': 0.835081727250549,\n",
       "   'r': 0.9782795084309803,\n",
       "   'f': 0.9010265859436694},\n",
       "  'obj': {'p': 0.7970244420828906,\n",
       "   'r': 0.8503401360544217,\n",
       "   'f': 0.8228195282501372},\n",
       "  'cc': {'p': 0.9600977198697068,\n",
       "   'r': 0.9616639477977161,\n",
       "   'f': 0.9608801955990219},\n",
       "  'conj': {'p': 0.775886524822695,\n",
       "   'r': 0.7698803659394793,\n",
       "   'f': 0.7728717767573297},\n",
       "  'dep': {'p': 0.4793103448275862,\n",
       "   'r': 0.5720164609053497,\n",
       "   'f': 0.5215759849906191},\n",
       "  'case': {'p': 0.9252909720037747,\n",
       "   'r': 0.9769179674526736,\n",
       "   'f': 0.9504038772213247},\n",
       "  'obl:arg': {'p': 0.8231780167264038,\n",
       "   'r': 0.8412698412698413,\n",
       "   'f': 0.8321256038647342},\n",
       "  'amod': {'p': 0.8530785562632697,\n",
       "   'r': 0.8294797687861272,\n",
       "   'f': 0.8411136696671551},\n",
       "  'mark': {'p': 0.8296422487223168,\n",
       "   'r': 0.9171374764595104,\n",
       "   'f': 0.8711985688729875},\n",
       "  'nmod': {'p': 0.811816192560175,\n",
       "   'r': 0.8629244844161885,\n",
       "   'f': 0.8365904990980156},\n",
       "  'advmod': {'p': 0.7545787545787546,\n",
       "   'r': 0.7907869481765835,\n",
       "   'f': 0.7722586691658857},\n",
       "  'acl': {'p': 0.7651612903225806,\n",
       "   'r': 0.7751633986928105,\n",
       "   'f': 0.7701298701298701},\n",
       "  'obl:mod': {'p': 0.7099236641221374,\n",
       "   'r': 0.7390728476821192,\n",
       "   'f': 0.7242050616482804},\n",
       "  'flat:name': {'p': 0.7916230366492146, 'r': 0.864, 'f': 0.8262295081967213},\n",
       "  'appos': {'p': 0.6915422885572139,\n",
       "   'r': 0.7354497354497355,\n",
       "   'f': 0.7128205128205128},\n",
       "  'cop': {'p': 0.8145161290322581,\n",
       "   'r': 0.9099099099099099,\n",
       "   'f': 0.8595744680851065},\n",
       "  'nsubj:pass': {'p': 0.8755555555555555,\n",
       "   'r': 0.9036697247706422,\n",
       "   'f': 0.8893905191873589},\n",
       "  'nummod': {'p': 0.8799668874172185,\n",
       "   'r': 0.9069965870307167,\n",
       "   'f': 0.8932773109243698},\n",
       "  'aux:tense': {'p': 0.8531468531468531,\n",
       "   'r': 0.7973856209150327,\n",
       "   'f': 0.8243243243243243},\n",
       "  'aux:pass': {'p': 0.9651162790697675,\n",
       "   'r': 0.9803149606299213,\n",
       "   'f': 0.97265625},\n",
       "  'obl:agent': {'p': 0.8938053097345132,\n",
       "   'r': 0.8632478632478633,\n",
       "   'f': 0.8782608695652174},\n",
       "  'acl:relcl': {'p': 0.6993464052287581,\n",
       "   'r': 0.722972972972973,\n",
       "   'f': 0.7109634551495018},\n",
       "  'xcomp': {'p': 0.7787356321839081,\n",
       "   'r': 0.811377245508982,\n",
       "   'f': 0.7947214076246335},\n",
       "  'expl:pass': {'p': 0.375, 'r': 0.5, 'f': 0.42857142857142855},\n",
       "  'ccomp': {'p': 0.46511627906976744,\n",
       "   'r': 0.7692307692307693,\n",
       "   'f': 0.5797101449275363},\n",
       "  'advcl': {'p': 0.6045197740112994,\n",
       "   'r': 0.6257309941520468,\n",
       "   'f': 0.6149425287356322},\n",
       "  'iobj': {'p': 0.8732394366197183,\n",
       "   'r': 0.8985507246376812,\n",
       "   'f': 0.8857142857142857},\n",
       "  'fixed': {'p': 0.7559055118110236, 'r': 0.75, 'f': 0.7529411764705882},\n",
       "  'parataxis': {'p': 0.673469387755102, 'r': 0.66, 'f': 0.6666666666666666},\n",
       "  'expl:subj': {'p': 0.49122807017543857,\n",
       "   'r': 0.8484848484848485,\n",
       "   'f': 0.6222222222222222},\n",
       "  'expl:comp': {'p': 0.391304347826087, 'r': 0.9, 'f': 0.5454545454545454},\n",
       "  'vocative': {'p': 0.625, 'r': 0.5555555555555556, 'f': 0.5882352941176471},\n",
       "  'flat:foreign': {'p': 0.16666666666666666,\n",
       "   'r': 0.18181818181818182,\n",
       "   'f': 0.17391304347826086}},\n",
       " 'tag_acc': 0.9411811367151866,\n",
       " 'lemma_acc': 0.9057236850728076,\n",
       " 'ents_p': 0.56601607347876,\n",
       " 'ents_r': 0.5730892182505086,\n",
       " 'ents_f': 0.5695306859205778,\n",
       " 'ents_per_type': {'ORG': {'p': 0.5675,\n",
       "   'r': 0.442495126705653,\n",
       "   'f': 0.4972617743702081},\n",
       "  'LOC': {'p': 0.5134831460674157,\n",
       "   'r': 0.5836526181353767,\n",
       "   'f': 0.5463239689181113},\n",
       "  'PER': {'p': 0.569164265129683,\n",
       "   'r': 0.47878787878787876,\n",
       "   'f': 0.5200789993416721},\n",
       "  'MISC': {'p': 0.6054545454545455,\n",
       "   'r': 0.8252788104089219,\n",
       "   'f': 0.6984792868379653}}}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer.score(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a358f45",
   "metadata": {},
   "source": [
    "### Training a custom spaCy model\n",
    "\n",
    "The training of a cupstom spaCy NER model can be done both with the command line interface (cli) or in a python script. Using the cli is ususally more optimzed. All the configuration of the training is defined in a coniguration file, which is a good practice for documentation, tracing and reproducibility.\n",
    "\n",
    "The configuration file can be generated on line using the [Quickstart](https://spacy.io/usage/training#quickstart)\n",
    "\n",
    "<img src=\"images/spacy_quickstart.jpg\" width=\"600\" >\n",
    "\n",
    "You can run the training process as a script using the train function (https://spacy.io/usage/training#api-train), specifying the configuration file and the directory in which to save the model as parameters. Once the training is complete, the best and last models are saved in the directory.\n",
    "\n",
    "#### Question\n",
    "> * Generate a training configuration file for a NER in French\n",
    "> * Add the correct path to the training and dev sets generated previously\n",
    "> * train a NER model\n",
    "> * Evaluate the model on the train, dev et test sets. Compare to the results reported in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bf90601b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m‚úî Created output directory: spacy_model\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ Saving to output directory: spacy_model\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m‚úî Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     74.36    0.45    0.30    0.84    0.00\n",
      "  0     200        898.63   2311.27   22.45   36.49   16.21    0.22\n",
      "  1     400        258.34   1485.19   39.12   56.57   29.89    0.39\n",
      "  1     600        655.84   1298.27   48.66   61.61   40.21    0.49\n",
      "  2     800        223.71   1189.86   50.39   53.02   48.00    0.50\n",
      "  3    1000        211.89   1026.38   49.94   62.74   41.47    0.50\n",
      "  5    1200        270.11    876.30   54.01   58.29   50.32    0.54\n",
      "  6    1400        366.54    877.53   53.38   62.61   46.53    0.53\n",
      "  8    1600        330.96    747.05   51.51   60.51   44.84    0.52\n",
      " 11    1800        486.99    782.90   53.89   58.01   50.32    0.54\n",
      " 14    2000        518.89    545.78   53.57   61.64   47.37    0.54\n",
      " 18    2200        365.42    519.75   52.64   63.31   45.05    0.53\n",
      " 23    2400        790.40    436.60   50.66   58.94   44.42    0.51\n",
      " 28    2600        467.50    451.42   48.04   60.13   40.00    0.48\n",
      " 33    2800        656.56    342.63   52.47   57.87   48.00    0.52\n",
      "\u001b[38;5;2m‚úî Saved pipeline to output directory\u001b[0m\n",
      "spacy_model/model-last\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "from spacy.cli.train import train\n",
    "# This is an auto-generated partial config. To use it with 'spacy train'\n",
    "# you can run spacy init fill-config to auto-fill all default settings:\n",
    "# python -m spacy init fill-config ./base_config.cfg ./config.cfg\n",
    "# NB: files need to have .spacy extension\n",
    "# NB: do not forget to set output_path, otherwise output is not saved\n",
    "train(\"./config.cfg\", output_path = \"./spacy_model\",\n",
    "      overrides={\"paths.train\": \"./corpus/train.spacy\", \n",
    "                 \"paths.dev\": \"./corpus/dev.spacy\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3982808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "nlp_trained = spacy.load('spacy_model/model-best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "89613bff-8ce8-42ed-a5be-ebfe675e4e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': None,\n",
       " 'token_p': None,\n",
       " 'token_r': None,\n",
       " 'token_f': None,\n",
       " 'ents_p': 0.7972889710412816,\n",
       " 'ents_r': 0.3760534728276664,\n",
       " 'ents_f': 0.5110584518167457,\n",
       " 'ents_per_type': {'ORG': {'p': 0.676056338028169,\n",
       "   'r': 0.4678362573099415,\n",
       "   'f': 0.5529953917050691},\n",
       "  'LOC': {'p': 0.8058823529411765,\n",
       "   'r': 0.3499361430395913,\n",
       "   'f': 0.48797862867319675},\n",
       "  'PER': {'p': 0.9424083769633508,\n",
       "   'r': 0.6545454545454545,\n",
       "   'f': 0.7725321888412017},\n",
       "  'MISC': {'p': 0.0, 'r': 0.0, 'f': 0.0}}}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer = Scorer(nlp_trained)\n",
    "predictions = list(map(lambda d: nlp_trained(d.text), doc_list))\n",
    "examples = list(map(Example, predictions,doc_list))\n",
    "scorer.score(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34932c83",
   "metadata": {},
   "source": [
    "### Zero-shot NER prediction with GLiNER\n",
    "\n",
    "\n",
    "[GLiNER](https://github.com/fastino-ai/GLiNER2/tree/main)  is a library that provides models for zero-shot named entity recognition. This means that[structured information extraction](https://github.com/fastino-ai/GLiNER2/blob/main/tutorial/3-json_extraction.md)structured information extraction, which means that the extracted information can be organised in a structured JSON format. GLiNER does not provide the location of entities in the text by default, but you can configure the model to output this information (```include_spans=True```). Finally, GLiNER enables entities to be overlapped and nested, which is not supported by the spaCy scorer. The spaCy [filter_spans](https://spacy.io/api/top-level#util.filter_spans) function can be used to remove overlapping entities for evaluation.\n",
    "\n",
    "#### Question\n",
    "> * Define the entities to extract from the text.\n",
    "> * Apply GLiNER on the dev and test sets\n",
    "> * Evaluate the models on the dev and test sets and compare to the results reported in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3f09c987",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type extractor to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üß† Model Configuration\n",
      "============================================================\n",
      "Encoder model      : microsoft/deberta-v3-base\n",
      "Counting layer     : count_lstm_v2\n",
      "Token pooling      : first\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'REPLACE'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[109]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m filter_spans\n\u001b[32m      4\u001b[39m nlp = spacy.blank(\u001b[33m\"\u001b[39m\u001b[33mfr\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# tokenizer only\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m doc_bin = \u001b[43mDocBin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mREPLACE\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m gold_docs = \u001b[38;5;28mlist\u001b[39m(doc_bin.get_docs(nlp.vocab))\n\u001b[32m      9\u001b[39m label_map = {\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# Define the entities here\u001b[39;00m\n\u001b[32m     11\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/nlpenv/lib/python3.13/site-packages/spacy/tokens/_serialize.py:275\u001b[39m, in \u001b[36mDocBin.from_disk\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m    267\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load the DocBin from a file (typically called .spacy).\u001b[39;00m\n\u001b[32m    268\u001b[39m \n\u001b[32m    269\u001b[39m \u001b[33;03mpath (str / Path): The file path.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    272\u001b[39m \u001b[33;03mDOCS: https://spacy.io/api/docbin#to_disk\u001b[39;00m\n\u001b[32m    273\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    274\u001b[39m path = ensure_path(path)\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file_:\n\u001b[32m    276\u001b[39m     \u001b[38;5;28mself\u001b[39m.from_bytes(file_.read())\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/pathlib/_local.py:537\u001b[39m, in \u001b[36mPath.open\u001b[39m\u001b[34m(self, mode, buffering, encoding, errors, newline)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m    536\u001b[39m     encoding = io.text_encoding(encoding)\n\u001b[32m--> \u001b[39m\u001b[32m537\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'REPLACE'"
     ]
    }
   ],
   "source": [
    "from gliner2 import GLiNER2\n",
    "extractor = GLiNER2.from_pretrained(\"fastino/gliner2-base-v1\")\n",
    "from spacy.util import filter_spans\n",
    "nlp = spacy.blank(\"fr\")  # tokenizer only\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "72e85b70-ad4a-4402-9731-3205c3352836",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_bin = DocBin().from_disk(\"./corpus/test.spacy\")\n",
    "gold_docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "\n",
    "label_map = {\n",
    "    # Define the entities here\n",
    "    \"ORG\": \"Business organizations and corporations\",\n",
    "    \"LOC\": \"Geographical places including cities\",\n",
    "    \"PER\": \"Names of individuals including executives\"\n",
    "}\n",
    "\n",
    "gliner_labels = list(label_map.values())\n",
    "reverse_map = {v: k for k, v in label_map.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "6e417769-5d1f-4889-aa7a-09c7ad9793d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples=[]\n",
    "for gold_doc in gold_docs:\n",
    "    text = gold_doc.text\n",
    "    \n",
    "    predictions = extractor.extract_entities(text, gliner_labels, include_spans=True)\n",
    "    pred_doc = nlp.make_doc(text)\n",
    "\n",
    "    spans = []\n",
    "    for gliner_label, entities in predictions[\"entities\"].items():\n",
    "        spacy_label = reverse_map.get(gliner_label)\n",
    "        if not spacy_label:\n",
    "            continue\n",
    "        for ent in entities:\n",
    "            start = ent[\"start\"]\n",
    "            end = ent[\"end\"]\n",
    "\n",
    "            span = pred_doc.char_span(start, end, label=spacy_label)\n",
    "            if span:\n",
    "                spans.append(span)\n",
    "\n",
    "    spans = filter_spans(spans)\n",
    "    pred_doc.ents = spans\n",
    "    examples.append(Example(pred_doc, gold_doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "f10fbf4d-1235-4886-9e7a-6ce4e42e8ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entities': {'Business organizations and corporations': [],\n",
       "  'Geographical places including cities': [],\n",
       "  'Names of individuals including executives': [{'text': 'Madame M√©lanie SAVARY',\n",
       "    'start': 83,\n",
       "    'end': 104}]}}"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = gold_docs[0].text\n",
    "extractor.extract_entities(text, gliner_labels, include_spans=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "dbf87e42-c409-4c13-8b58-75e219bc4f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "9232500e-7593-41e5-bebc-dc2dfa8f855d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "et|\t\tet|\t\t|\t\t\n",
      "L‚Äô|\t\tL‚ÄôOffice|\t\t|\t\tORG\n",
      "Office|\t\tCommunautaire|\t\t|\t\tORG\n",
      "Communautaire|\t\td‚ÄôAnimations|\t\t|\t\tORG\n",
      "d‚Äô|\t\tet|\t\t|\t\tORG\n",
      "Animations|\t\tde|\t\t|\t\tORG\n",
      "et|\t\tLoisirs|\t\t|\t\tORG\n",
      "de|\t\t,|\t\t|\t\t\n",
      "Loisirs|\t\t¬´|\t\t|\t\t\n",
      ",|\t\tL‚ÄôOCAL|\t\t|\t\tORG\n",
      "¬´|\t\t¬ª|\t\t|\t\t\n",
      "L‚Äô|\t\t,|\t\t|\t\t\n",
      "OCAL|\t\trepr√©sent√©|\t\t|\t\t\n",
      "¬ª|\t\tpar|\t\t|\t\t\n",
      ",|\t\tMadame|\t\t|\t\tPER\n",
      "repr√©sent√©|\t\tM√©lanie|\t\t|\t\tPER\n",
      "par|\t\tSAVARY|\t\t|\t\tPER\n",
      "Madame|\t\t,|\t\tPER|\t\t\n",
      "M√©lanie|\t\tpr√©sidente|\t\tPER|\t\t\n",
      "SAVARY|\t\t,|\t\tPER|\t\t\n",
      ",|\t\til|\t\t|\t\t\n",
      "pr√©sidente|\t\test|\t\t|\t\t\n",
      ",|\t\tconvenu|\t\t|\t\t\n",
      "il|\t\tce|\t\t|\t\t\n",
      "est|\t\tqui|\t\t|\t\t\n",
      "convenu|\t\tsuit|\t\t|\t\t\n",
      "ce|\t\t.|\t\t|\t\t\n"
     ]
    }
   ],
   "source": [
    "x = examples[0].x\n",
    "y = examples[0].y\n",
    "for a,b in zip(x,y):\n",
    "    print(a,b, a.ent_type_, b.ent_type_, sep=\"|\\t\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "79c72726-f2b7-4e2a-a40e-cc3a16df5a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': None,\n",
       " 'token_p': None,\n",
       " 'token_r': None,\n",
       " 'token_f': None,\n",
       " 'ents_p': 0.27181208053691275,\n",
       " 'ents_r': 0.2709030100334448,\n",
       " 'ents_f': 0.271356783919598,\n",
       " 'ents_per_type': {'PER': {'p': 0.4326923076923077,\n",
       "   'r': 0.3629032258064516,\n",
       "   'f': 0.39473684210526316},\n",
       "  'ORG': {'p': 0.3050847457627119,\n",
       "   'r': 0.14634146341463414,\n",
       "   'f': 0.1978021978021978},\n",
       "  'LOC': {'p': 0.13333333333333333,\n",
       "   'r': 0.34615384615384615,\n",
       "   'f': 0.1925133689839572}}}"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer = Scorer(nlp_trained)\n",
    "scorer.score(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "a4ca3f35-e101-4f28-ad61-2ada8081009b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[et,\n",
       " L‚ÄôOffice,\n",
       " Communautaire,\n",
       " d‚ÄôAnimations,\n",
       " et,\n",
       " de,\n",
       " Loisirs,\n",
       " ,,\n",
       " ¬´,\n",
       " L‚ÄôOCAL,\n",
       " ¬ª,\n",
       " ,,\n",
       " repr√©sent√©,\n",
       " par,\n",
       " Madame,\n",
       " M√©lanie,\n",
       " SAVARY,\n",
       " ,,\n",
       " pr√©sidente,\n",
       " ,,\n",
       " il,\n",
       " est,\n",
       " convenu,\n",
       " ce,\n",
       " qui,\n",
       " suit,\n",
       " .]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(gold_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941af042-e580-4b38-9b4e-6d498583c5b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "a27d09fd-210d-4ac1-bb14-91f15e7dca80",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'endswith'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[265]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m entities = \u001b[43mextractor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_entities\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgold_docs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mORG\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPER\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/nlpenv/lib/python3.13/site-packages/gliner2/inference/engine.py:1157\u001b[39m, in \u001b[36mGLiNER2.extract_entities\u001b[39m\u001b[34m(self, text, entity_types, threshold, format_results, include_confidence, include_spans)\u001b[39m\n\u001b[32m   1155\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Extract entities from text.\"\"\"\u001b[39;00m\n\u001b[32m   1156\u001b[39m schema = \u001b[38;5;28mself\u001b[39m.create_schema().entities(entity_types)\n\u001b[32m-> \u001b[39m\u001b[32m1157\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_confidence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_spans\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/nlpenv/lib/python3.13/site-packages/gliner2/inference/engine.py:1150\u001b[39m, in \u001b[36mGLiNER2.extract\u001b[39m\u001b[34m(self, text, schema, threshold, format_results, include_confidence, include_spans)\u001b[39m\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, schema, threshold: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m0.5\u001b[39m,\n\u001b[32m   1147\u001b[39m             format_results: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m, include_confidence: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1148\u001b[39m             include_spans: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> Dict:\n\u001b[32m   1149\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Extract from single text.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_confidence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_spans\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/nlpenv/lib/python3.13/site-packages/torch/utils/_contextlib.py:124\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/nlpenv/lib/python3.13/site-packages/gliner2/inference/engine.py:406\u001b[39m, in \u001b[36mGLiNER2.batch_extract\u001b[39m\u001b[34m(self, texts, schemas, batch_size, threshold, num_workers, format_results, include_confidence, include_spans)\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m text:\n\u001b[32m    405\u001b[39m     text = \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mendswith\u001b[49m((\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m!\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m?\u001b[39m\u001b[33m'\u001b[39m)):\n\u001b[32m    407\u001b[39m     text = text + \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    408\u001b[39m normalized.append(text)\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'endswith'"
     ]
    }
   ],
   "source": [
    "entities = extractor.extract_entities(list(gold_docs[0]), [\"ORG\",\"PER\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb93035-8da1-4c77-86af-942c540b7093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gliner2 import GLiNER\n",
    "\n",
    "# Load GLiNER2 model\n",
    "model_name = \"urchade/gliner2-base\"\n",
    "model = GLiNER.from_pretrained(model_name)\n",
    "\n",
    "# Your pre-tokenized input\n",
    "tokens = [\"John\", \"lives\", \"in\", \"New\", \"York\", \".\"]\n",
    "# Build the text and track character offsets\n",
    "text = \"\"\n",
    "offsets = []\n",
    "pos = 0\n",
    "for tok in tokens:\n",
    "    if text:  # add space before token except first\n",
    "        text += \" \"\n",
    "        pos += 1\n",
    "    start = pos\n",
    "    text += tok\n",
    "    end = pos + len(tok)\n",
    "    offsets.append((start, end))\n",
    "    pos = end\n",
    "\n",
    "# Run GLiNER2 inference\n",
    "predictions = model.predict_entities(text)\n",
    "\n",
    "# Align predictions to your tokens\n",
    "def align_predictions(predictions, tokens, offsets):\n",
    "    aligned = [[] for _ in tokens]\n",
    "    for pred in predictions:\n",
    "        start, end, label = pred[\"start\"], pred[\"end\"], pred[\"label\"]\n",
    "        for i, (tok_start, tok_end) in enumerate(offsets):\n",
    "            # Token is inside the predicted span\n",
    "            if tok_start >= start and tok_end <= end:\n",
    "                aligned[i].append(label)\n",
    "    return aligned\n",
    "\n",
    "aligned_labels = align_predictions(predictions, tokens, offsets)\n",
    "\n",
    "# Show results\n",
    "for token, labels in zip(tokens, aligned_labels):\n",
    "    print(f\"{token}: {labels}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "nlpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
